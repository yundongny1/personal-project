{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mCA2mboGoav"
      },
      "source": [
        "\n",
        "## Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah2MgTw0xQtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586209f4-f66f-4778-830f-cddf77bf84c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
            "(EOF or read error, treating as \"[N]one\" ...)\n"
          ]
        }
      ],
      "source": [
        "#@title  <-- Run Me: (Hidden) Installing Spark\n",
        "%%bash \n",
        "\n",
        "## Setup Spark on Colab\n",
        "pip install -q pyspark\n",
        "apt-get -qq install -y openjdk-8-jdk-headless\n",
        "\n",
        "## Setup port-forwarding\n",
        "\n",
        "# Download ngrok\n",
        "wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# Unload ngrok\n",
        "unzip -q ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVm_ncdJx2YG"
      },
      "outputs": [],
      "source": [
        "#@title  <-- Run Me: (Hidden) Environment Variable Setup\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E0AMIX1yAZg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "98cef9ab-c5ff-4c7a-d119-8fb5d8f412dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9794665be160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pyspark \n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd    \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Set configuration scope to be local and use port 4050\n",
        "config_scope = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# Create the connection to a Spark cluster\n",
        "sc = pyspark.SparkContext(conf = config_scope)\n",
        "\n",
        "# Create a session to programmatically build Spark RDD, DataFrame and DataSet\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjdp8pAw7Isp"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6yWKFa57Z5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0907b7-e86c-4849-936b-39cf44948830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VePiHW_ezwGT"
      },
      "outputs": [],
      "source": [
        "# Read in data\n",
        "TRAIN_DIR = \"drive/MyDrive/STAT480-Group Project/dataset/track2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/training.txt')\n",
        "training = training.selectExpr('_c0 as Click', '_c1 as Impression', '_c2 as AdURL', '_c3 as AdId', '_c4 as AdvId', \n",
        "                    '_c5 as Depth', '_c6 as Pos', '_c7 as QId', '_c8 as KeyId', '_c9 as TitleId', \n",
        "                    '_c10 as DescId', '_c11 as UId')\n",
        "training.show(10)"
      ],
      "metadata": {
        "id": "WFfBhv9btAWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "75ff4244-93ca-4710-e704-9d052bf92d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-da721152bd2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/training.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m training = training.selectExpr('_c0 as Click', '_c1 as Impression', '_c2 as AdURL', '_c3 as AdId', '_c4 as AdvId', \n\u001b[1;32m      3\u001b[0m                     \u001b[0;34m'_c5 as Depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_c6 as Pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_c7 as QId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_c8 as KeyId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_c9 as TitleId'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     '_c10 as DescId', '_c11 as UId')\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "purchasekeywordid = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/purchasedkeywordid_tokensid.txt')\n",
        "purchasekeywordid = purchasekeywordid.selectExpr('_c0 as KeyId', '_c1 as PurchaseKeyword')\n",
        "purchasekeywordid.show(10)"
      ],
      "metadata": {
        "id": "M3yhizsqsWT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queryid = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/queryid_tokensid.txt')\n",
        "queryid = queryid.selectExpr('_c0 as QId', '_c1 as Query')\n",
        "queryid.show(10)"
      ],
      "metadata": {
        "id": "ZFTjVaGcsRVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptionid = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/descriptionid_tokensid.txt')\n",
        "descriptionid = descriptionid.selectExpr('_c0 as DescId', '_c1 as Description')\n",
        "descriptionid.show(10)"
      ],
      "metadata": {
        "id": "OyRN-FFOsbBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userid = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/userid_profile.txt')\n",
        "userid = userid.selectExpr('_c0 as UId', '_c1 as Gender', '_c2 as Age')\n",
        "userid.show(10)"
      ],
      "metadata": {
        "id": "AuqkoheKsdnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titleid = spark.read.option(\"header\",\"false\").option(\"delimiter\",\"\\t\").csv(TRAIN_DIR+'/titleid_tokensid.txt')\n",
        "titleid = titleid.selectExpr('_c0 as TitleId', '_c1 as Titile')"
      ],
      "metadata": {
        "id": "tGWYuesax64I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titleid.show(10)"
      ],
      "metadata": {
        "id": "Ea_rhN6wy044"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zWt3H_dGfqN"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gender and Age"
      ],
      "metadata": {
        "id": "3CZyWY4h3qEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aNqIJmmEtOYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "82c87efd-1cbe-4a51-f739-020a51a25b58"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd40be1eaffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEnnx3iqtZsw"
      },
      "outputs": [],
      "source": [
        "# label encode\n",
        "Gender_indexer = StringIndexer(inputCol='Gender', outputCol='Gender_num').fit(userid)\n",
        "Age_indexer = StringIndexer(inputCol='Age', outputCol='Age_num').fit(userid)\n",
        "\n",
        "\n",
        "userid = Gender_indexer.transform(userid)\n",
        "userid = Age_indexer.transform(userid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46v1ivUhNlQN"
      },
      "outputs": [],
      "source": [
        "# one-hot encoder\n",
        "Gender_onehoter = OneHotEncoder(inputCol='Gender_num', outputCol='Gender_vector')\n",
        "Age_onehoter = OneHotEncoder(inputCol='Age_num', outputCol='Age_vector')\n",
        "\n",
        "\n",
        "ohe1 = Gender_onehoter.fit(userid)\n",
        "ohe2 = Age_onehoter.fit(userid)\n",
        "\n",
        "\n",
        "userid = ohe1.transform(userid)\n",
        "userid = ohe2.transform(userid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userid.show(10)"
      ],
      "metadata": {
        "id": "aOi_SV6_5743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Click Through Rate"
      ],
      "metadata": {
        "id": "4rGuNqRY6LHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# average click through rate\n",
        "temp_df1 = training.groupBy(\"AdvId\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Advertiser\"))\n",
        "temp_df1.show(10)"
      ],
      "metadata": {
        "id": "1Z-t5OTX6egE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df2 = training.groupBy(\"AdID\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Ad\"))\n",
        "temp_df2.show(10)"
      ],
      "metadata": {
        "id": "l93Tg32oN5Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df3 = training.groupBy(\"QId\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Query\"))\n",
        "temp_df3.show(10)"
      ],
      "metadata": {
        "id": "79Ze-r9KACM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df4 = training.groupBy(\"KeyId\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Key\"))\n",
        "temp_df4.show(10)"
      ],
      "metadata": {
        "id": "kgLu383dOhdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df5 = training.groupBy(\"TitleId\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Title\"))\n",
        "temp_df5.show(10)"
      ],
      "metadata": {
        "id": "6m_6lrg2OmX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df6 = training.groupBy(\"DescId\").agg((avg(\"Click\")/count(\"Click\")).alias(\"AvgClick_Desciption\"))\n",
        "temp_df6.show(10)"
      ],
      "metadata": {
        "id": "iIjdGNNlOxMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Join tables together"
      ],
      "metadata": {
        "id": "k3gPCDBjO-Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = training.join(userid, on=\"UId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df1, on=\"AdvId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df2, on=\"AdId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df3, on=\"QId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df4, on=\"KeyId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df5, on=\"TitleId\", how=\"inner\")\n",
        "train_df = train_df.join(temp_df6, on=\"DescId\", how=\"inner\")\n",
        "train_df.show(10)"
      ],
      "metadata": {
        "id": "Nrkk4Bn4bEmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6uvtaROOsFH"
      },
      "source": [
        "## Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-rbmIvGNmi0"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "train_assembler = VectorAssembler(inputCols=['Impression', 'Depth', 'Pos', \n",
        "                        'Gender_vector', 'Age_vector',\n",
        "                        'AvgImp_Advertiser', 'AvgClick_Ad',\n",
        "                        'AvgClick_Query', 'AvgClick_Key',\n",
        "                        'AvgClick_Title', 'AvgClick_Desciption'],\n",
        "                 outputCol='features')\n",
        "train_df = train_assembler.transform(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJyw4hPGOze8"
      },
      "outputs": [],
      "source": [
        "# split the dataset\n",
        "dataset = train_df.select(['features', 'Click'])\n",
        "train, test = dataset.randomSplit([0.8, 0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u1sXZeFO9sj"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(labelCol = 'Click').fit(train)\n",
        "\n",
        "train_pred = log_reg.evaluate(train).predictions\n",
        "\n",
        "train_pred.filter(train_pred['Click'] == 1).filter(train_pred['prediction'] == 1).select(['Click', 'prediction', 'probability']).show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G86YruCYPdHT"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_xOke2gPZfc"
      },
      "outputs": [],
      "source": [
        "result_lr = log_reg.evaluate(test).predictions\n",
        "result_lr.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bBZ8Y6vPif9"
      },
      "outputs": [],
      "source": [
        "tp_lr = result_lr[(result_lr.label == 1) & (result_lr.prediction == 1)].count()\n",
        "tn_lr = result_lr[(result_lr.label == 0) & (result_lr.prediction == 1)].count()\n",
        "fp_lr = result_lr[(result_lr.label == 0) & (result_lr.prediction == 1)].count()\n",
        "fn_lr = result_lr[(result_lr.label == 1) & (result_lr.prediction == 0)].count()\n",
        "\n",
        "print('tp is : %f'%(tp_lr))\n",
        "print('tn is : %f'%(tn_lr))\n",
        "print('fp is : %f'%(fp_lr))\n",
        "print('fn is : %f'%(fn_lr))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "print('test accuracy is : %f'%((tp_lr+tn_lr)/(tp_lr+tn_lr+fp_lr+fn_lr)))\n",
        "\n",
        "# Recall\n",
        "print('test accuracy is : %f'%(tp_lr/(tp_lr+fn_lr)))\n",
        "\n",
        "# Precision\n",
        "print('test accuracy is : %f'%(tp_lr/(tp_lr+fp_lr)))"
      ],
      "metadata": {
        "id": "2t182OldjVBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_lr = result_lr.select('Click','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['Click']))).collect()\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "y_score, y_true = zip(*preds_lr)\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)"
      ],
      "metadata": {
        "id": "G0e8B1sHDwSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "Uw92gNs9DyWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "fLShxmXMrnyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Click')\n",
        "rf_model = rf.fit(train)"
      ],
      "metadata": {
        "id": "ta36nlfFrtKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_rf = rf_model.transform(test).select(\"Click\", \"probability\").rdd.map(lambda row: (float(row['probability'][1]), float(row['Click']))).collect()\n",
        "result_rf.show(5)"
      ],
      "metadata": {
        "id": "sK4_uxCq_Oy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp_rf = result_rf[(result_rf.label == 1) & (result_rf.prediction == 1)].count()\n",
        "tn_rf = result_rf[(result_rf.label == 0) & (result_rf.prediction == 1)].count()\n",
        "fp_rf = result_rf[(result_rf.label == 0) & (result_rf.prediction == 1)].count()\n",
        "fn_rf = result_rf[(result_rf.label == 1) & (result_rf.prediction == 0)].count()\n",
        "\n",
        "print('tp is : %f'%(tp_rf))\n",
        "print('tn is : %f'%(tn_rf))\n",
        "print('fp is : %f'%(fp_rf))\n",
        "print('fn is : %f'%(fn_rf))"
      ],
      "metadata": {
        "id": "Fpg6wKNiHMq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "print('test accuracy is : %f'%((tp_rf+tn_rf)/(tp_rf+tn_rf+fp_rf+fn_rf)))\n",
        "\n",
        "# Recall\n",
        "print('test accuracy is : %f'%(tp_rf/(tp_rf+fn_rf)))\n",
        "\n",
        "# Precision\n",
        "print('test accuracy is : %f'%(tp_rf/(tp_rf+fp_rf)))"
      ],
      "metadata": {
        "id": "Vy0FIS8QHP-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "X-uVUyLjjj_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler \n",
        "from pyspark.ml.classification import NaiveBayes \n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "train_nb_assembler = VectorAssembler(inputCols=['UId',\n",
        "                        'QId', 'AdId',\n",
        "                        'KeyId', 'Pos'],\n",
        "                 outputCol='features')\n",
        "train_nb_df = train_nb_assembler.transform(training)"
      ],
      "metadata": {
        "id": "pnloDtGKj-qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "dataset_nb = train_nb_df.select(['features', 'Click'])\n",
        "train_nb, test_nb = dataset_nb.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "JkT2en9go67p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayes(modelType='multinomial')\n",
        "nbmodel = nb.fit(train_nb)"
      ],
      "metadata": {
        "id": "rxtXZszVpJYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "rpcg-ojgpWGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_nb = nbmodel.transform(test_nb)\n",
        "result_nb.show(5)"
      ],
      "metadata": {
        "id": "ebuzpRthpau4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jlr8nLsrJil"
      },
      "outputs": [],
      "source": [
        "tp_nb = result_nb[(result_nb.label == 1) & (result_nb.prediction == 1)].count()\n",
        "tn_nb = result_nb[(result_nb.label == 0) & (result_nb.prediction == 1)].count()\n",
        "fp_nb = result_nb[(result_nb.label == 0) & (result_nb.prediction == 1)].count()\n",
        "fn_nb = result_nb[(result_nb.label == 1) & (result_nb.prediction == 0)].count()\n",
        "\n",
        "print('tp is : %f'%(tp_nb))\n",
        "print('tn is : %f'%(tn_nb))\n",
        "print('fp is : %f'%(fp_nb))\n",
        "print('fn is : %f'%(fn_nb))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "print('test accuracy is : %f'%((tp_nb+tn_nb)/(tp_nb+tn_nb+fp_nb+fn_nb)))\n",
        "\n",
        "# Recall\n",
        "print('test accuracy is : %f'%(tp_nb/(tp_nb+fn_nb)))\n",
        "\n",
        "# Precision\n",
        "print('test accuracy is : %f'%(tp_nb/(tp_nb+fp_nb)))"
      ],
      "metadata": {
        "id": "XTvodLfUrNlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_nb = result_nb.select('Click','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['Click']))).collect()\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "y_score, y_true = zip(*preds_nb)\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)"
      ],
      "metadata": {
        "id": "he0HWC9mAs2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "nwv39D3jAu5s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4mCA2mboGoav",
        "kjdp8pAw7Isp"
      ],
      "name": "Model_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}